{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "# NLTK imports\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "from nltk.translate.nist_score import sentence_nist\n",
    "\n",
    "# others\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import load_metric\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# sklearn imports\n",
    "import sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTCheckDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.references, self.candidates, self.scores, self.labels = self.import_data(self.data_path)\n",
    "        self.samples = self.generate_features(self.references, self.candidates)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.tensor(self.samples.loc[idx]).float()\n",
    "        label = self.labels[idx]\n",
    "        return (sample, label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def import_data(self, file):\n",
    "        # English references\n",
    "        references = []\n",
    "        # Translated English sentences\n",
    "        candidates = []\n",
    "        # Bleu scores for translations\n",
    "        scores = []\n",
    "        # Labels indicating human (H-->0) or machine translation (M-->1)\n",
    "        labels = []\n",
    "        with open(file) as f:\n",
    "            lines = f.readlines()\n",
    "            for i, line in enumerate(lines):\n",
    "                line = line.strip('\\n')\n",
    "                if i % 6 == 1:\n",
    "                    references.append(line)\n",
    "                elif i % 6 == 2:\n",
    "                    candidates.append(line)\n",
    "                elif i % 6 == 3:\n",
    "                    scores.append(float(line))\n",
    "                elif i % 6 == 4:\n",
    "                    if line == 'H':\n",
    "                        labels.append(0)\n",
    "                    else:\n",
    "                        labels.append(1)\n",
    "        return references, candidates, scores, labels\n",
    "    \n",
    "    def generate_features(self, references, candidates):\n",
    "        # define empty arrays for metrics\n",
    "        blue_1_ind, blue_2_ind, blue_3_ind, blue_4_ind, blue_2_cumu, blue_3_cumu, blue_4_cumu = [], [], [], [], [], [], []\n",
    "        gleu_default, nist_defualt = [], []\n",
    "\n",
    "        # compute 9 different metrics\n",
    "        for ref, cand in zip(references, candidates):\n",
    "            ref_split = ref.split(' ')\n",
    "            ref_input = [ref_split]\n",
    "            cand_input = cand.split(' ')\n",
    "            # individual bleu scores\n",
    "            blue_1_ind.append(sentence_bleu(ref_input, cand_input, weights=(1, 0, 0, 0)))\n",
    "            blue_2_ind.append(sentence_bleu(ref_input, cand_input, weights=(0, 1, 0, 0)))\n",
    "            blue_3_ind.append(sentence_bleu(ref_input, cand_input, weights=(0, 0, 1, 0)))\n",
    "            blue_4_ind.append(sentence_bleu(ref_input, cand_input, weights=(0, 0, 0, 1)))\n",
    "            # cumulative bleu scores\n",
    "            blue_2_cumu.append(sentence_bleu(ref_input, cand_input, weights=(0.5, 0.5, 0, 0)))\n",
    "            blue_3_cumu.append(sentence_bleu(ref_input, cand_input, weights=(0.33, 0.33, 0.33, 0)))\n",
    "            blue_4_cumu.append(sentence_bleu(ref_input, cand_input, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "            # other scores\n",
    "            gleu_default.append(sentence_gleu(ref_input, cand_input))\n",
    "            nist_defualt.append(sentence_nist(ref_input, cand_input))\n",
    "            #ribes_defualt.append(sentence_ribes(ref_input, cand_input))\n",
    "\n",
    "        # pass metrics to dataframe and return\n",
    "        feature_dict = {\"blue_1_ind\": blue_1_ind, \"blue_2_ind\": blue_2_ind, \"blue_3_ind\": blue_3_ind,\n",
    "                        \"blue_4_ind\": blue_4_ind, \"blue_2_cumu\": blue_2_cumu, \"blue_3_cumu\": blue_3_cumu,\n",
    "                        \"blue_4_cumu\": blue_4_cumu, \"gleu_default\": gleu_default, \"nist_defualt\": nist_defualt}\n",
    "        features = pd.DataFrame(feature_dict)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train and test data\n",
    "train_data = MTCheckDataset(data_path=\"train.txt\")\n",
    "test_data = MTCheckDataset(data_path=\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train and test dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add optuna hyperparm search\n",
    "class FFNN(LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.hidden = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.out = nn.Linear(self.hidden_dim, self.out_dim)\n",
    "        self.activation = F.tanh\n",
    "        self.droput = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.droput(x)\n",
    "        x = F.sigmoid(self.out(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffnn = FFNN(input_dim=9, hidden_dim=256, out_dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "optimizer = torch.optim.Adam(ffnn.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train_loss: 0.58, f1_score: 0.73, accuracy: 0.71\n",
      "epoch 2, train_loss: 0.59, f1_score: 0.71, accuracy: 0.66\n",
      "epoch 3, train_loss: 0.58, f1_score: 0.74, accuracy: 0.75\n",
      "epoch 4, train_loss: 0.58, f1_score: 0.73, accuracy: 0.74\n",
      "epoch 5, train_loss: 0.57, f1_score: 0.73, accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "num_epochs = 5\n",
    "total = len(test_data)\n",
    "for epoch in range(num_epochs):\n",
    "    losses = list()\n",
    "    for batch in train_loader:\n",
    "        X, y = batch\n",
    "        # 1. forward\n",
    "        predicted_vector = ffnn(X)\n",
    "        # 2. compute objective function\n",
    "        J = loss(predicted_vector, y)\n",
    "        # 3. clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # 4. accumulate partial derivatives of J w.r.t to params\n",
    "        J.backward()\n",
    "        # 5. step in opp direction of gradient\n",
    "        optimizer.step()     \n",
    "        losses.append(J.item())\n",
    "\n",
    "    train_loss = torch.tensor(losses).mean()\n",
    "\n",
    "    all_labels, all_preds = list(), list()\n",
    "    for batch in test_loader:\n",
    "        X, y = batch\n",
    "        # 1. forward\n",
    "        predicted_vector = ffnn(X)\n",
    "        # for accuracy\n",
    "        predicted_labels = torch.argmax(predicted_vector, dim=1)\n",
    "        # for f1 score\n",
    "        all_labels.append(y.tolist())\n",
    "        all_preds.append(predicted_labels.tolist())\n",
    "\n",
    "    # flatten the lists\n",
    "    all_labels = [item for sublist in all_labels for item in sublist]\n",
    "    all_preds = [item for sublist in all_preds for item in sublist]\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    print(f'epoch {epoch+1}, train_loss: {train_loss:.2f}, f1_score: {f1:.2f}, accuracy: {acc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 748,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC()\n",
    "clf.fit(train_data.samples, train_data.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(test_data.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7500000000000001\n"
     ]
    }
   ],
   "source": [
    "f1_score = sklearn.metrics.f1_score(test_data.labels, predictions)\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
